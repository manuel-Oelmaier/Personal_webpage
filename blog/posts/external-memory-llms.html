<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>External Memory for LLMs | Manuel Oelmaier</title>
    <link rel="stylesheet" href="../../style.css">
    <link rel="stylesheet" href="../blog.css">
    <style>
        .post-container {
            max-width: 800px;
            margin: 0 auto;
            text-align: left;
            padding: 4rem 2rem;
            line-height: 1.8;
        }
        .post-meta {
            color: hsl(185, 49%, 40%);
            margin-bottom: 2rem;
            font-size: 1.1rem;
        }
        h1 { font-size: 3rem; margin-bottom: 1rem; text-align: center; }
        h2 { font-size: 2.2rem; margin-top: 3rem; margin-bottom: 1rem; border-bottom: 1px solid rgba(255, 255, 255, 0.1); padding-bottom: 0.5rem; }
        p { margin-bottom: 1.5rem; color: var(--normal-TextColor); font-size: 1.2rem; }
        .back-link {
            display: inline-block;
            margin-bottom: 2rem;
        }
        .nav-wrapper { text-align: center; padding-top: 4rem; }
    </style>
</head>
<body>
<main>
    <div class="nav-wrapper">
        <a href="../index.html" class="back-link">← Back to Blog</a>
    </div>

    <article class="post-container">
        <header>
            <h1>Scaling Context: The State of External Memory in LLMs</h1>
            <div class="post-meta">Published on February 3, 2026 • Research Note</div>
        </header>

        <p>
            As Large Language Models (LLMs) scale, the quadratic cost of self-attention becomes the primary bottleneck for long-context reasoning. For my current thesis research, I'm exploring how we can implement external memory systems and sparse attention variants to maintain performance while keeping VRAM usage under control—specifically targeting consumer-grade hardware with 16GB limits.
        </p>

        <h2>The KV Cache Problem</h2>
        <p>
            The Key-Value (KV) cache grows linearly with sequence length. In a standard Transformer, this quickly leads to Out-of-Memory (OOM) errors. Recent research has focused on "token eviction"—deciding which parts of the past are worth remembering.
        </p>

        <h2>Key Research Papers (2023-2025)</h2>
        <table class="research-table">
            <thead>
                <tr>
                    <th>Paper</th>
                    <th>Core Idea</th>
                    <th>VRAM Impact</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><strong>H₂O</strong></td>
                    <td>Heavy-Hitter Oracle: Keep tokens with highest attention scores.</td>
                    <td>Up to 5x reduction</td>
                </tr>
                <tr>
                    <td><strong>StreamingLLM</strong></td>
                    <td>Attention Sinks: Keep initial tokens + sliding window.</td>
                    <td>Fixed VRAM usage</td>
                </tr>
                <tr>
                    <td><strong>LightTransfer</strong></td>
                    <td>Hybrid Layers: Identify "lazy layers" and use sparse attention.</td>
                    <td>2.17x throughput gain</td>
                </tr>
            </tbody>
        </table>

        <h2>Next Steps: Experiments</h2>
        <p>
            My experiments will focus on <strong>LightTransfer (2025)</strong>, as it offers a promising balance for 16GB VRAM setups by identifying which layers actually require dense attention and which can be offloaded to more efficient streaming mechanisms.
        </p>
    </article>

    <footer>
        CC0 — Free to share, free to use. Created by Manuel Oelmaier.
    </footer>
</main>
</body>
</html>
